from flask import Flask, request, jsonify
import json
import random
import os

# --- Configurazione ---
Q_TABLE_FILE = "q_table.json"
# Hyperparameters per il Q-learning
ALPHA = 0.1  # Tasso di apprendimento
GAMMA = 0.9  # Fattore di sconto per ricompense future
EPSILON = 0.1  # Tasso di esplorazione (10% di mosse casuali)

app = Flask(__name__)

# --- Logica dell'Agente Q-learning ---

def get_state_key(board):
    """Converte la board (lista di liste) in una stringa immutabile per usarla come chiave."""
    return "".join("".join(row) for row in board).replace("", "_")

def get_available_actions(board):
    """Restituisce una lista di mosse possibili (riga, colonna)."""
    actions = []
    for r, row in enumerate(board):
        for c, cell in enumerate(row):
            if cell == "":
                actions.append((r, c))
    return actions

def load_q_table():
    """Carica la Q-table da un file JSON, se esiste."""
    if os.path.exists(Q_TABLE_FILE):
        with open(Q_TABLE_FILE, 'r') as f:
            return json.load(f)
    return {}

def save_q_table(q_table):
    """Salva la Q-table in un file JSON."""
    with open(Q_TABLE_FILE, 'w') as f:
        json.dump(q_table, f)

q_table = load_q_table()

def choose_action(board):
    """Sceglie un'azione usando la strategia epsilon-greedy."""
    available_actions = get_available_actions(board)
    if not available_actions:
        return None

    if random.uniform(0, 1) < EPSILON:
        # Esplorazione: scegli una mossa casuale
        return random.choice(available_actions)
    else:
        # Sfruttamento: scegli la mossa migliore dalla Q-table
        state_key = get_state_key(board)
        state_q_values = q_table.get(state_key, {})

        if not state_q_values:
            # Se lo stato è sconosciuto, scegli una mossa casuale
            return random.choice(available_actions)

        max_q = -float('inf')
        best_action = None

        for r, c in available_actions:
            action_key = f"{r},{c}"
            q_val = state_q_values.get(action_key, 0.0)
            if q_val > max_q:
                max_q = q_val
                best_action = (r, c)
        
        return best_action if best_action else random.choice(available_actions)

# --- Endpoint Flask API ---

@app.route('/move', methods=['POST'])
def get_move():
    data = request.get_json()
    if not data or 'board' not in data:
        return jsonify({'error': 'Invalid request format'}), 400

    board = data['board']
    
    action = choose_action(board)
    
    if action:
        print(f"Board ricevuta. Mossa scelta dall'agente: {action}")
        return jsonify({'move': [action[0], action[1]]})
    else:
        return jsonify({'error': 'No available moves'}), 400

# --- Training (non essenziale per l'esecuzione, ma necessario per l'apprendimento) ---
# Questa parte è qui per mostrarti come l'agente potrebbe essere addestrato.
# Puoi eseguire questo script con un argomento "train" per avviare il training.
# Esempio: python server.py train

def check_game_over(board):
    """Controlla se c'è un vincitore o un pareggio."""
    lines = []
    lines.extend(board)  # Righe
    lines.extend(list(zip(*board)))  # Colonne
    lines.append([board[i][i] for i in range(3)])  # Diagonale 1
    lines.append([board[i][2-i] for i in range(3)]) # Diagonale 2

    for line in lines:
        if line.count('X') == 3: return 'X'
        if line.count('O') == 3: return 'O'

    if not get_available_actions(board):
        return 'draw'
    return None

def train(episodes=50000):
    """Addestra l'agente facendolo giocare contro se stesso (o un avversario casuale)."""
    global q_table
    print(f"Inizio training per {episodes} episodi...")

    for i in range(episodes):
        board = [["" for _ in range(3)] for _ in range(3)]
        history = []
        turn = 'X' # Inizia il giocatore 'X' (casuale)
        
        while check_game_over(board) is None:
            state_key = get_state_key(board)
            available_actions = get_available_actions(board)
            
            if turn == 'O': # Turno dell'agente Q-learning
                action = choose_action(board)
            else: # Turno del giocatore casuale
                action = random.choice(available_actions)

            r, c = action
            board[r][c] = turn
            action_key = f"{r},{c}"
            
            if turn == 'O':
                history.append({'state': state_key, 'action': action_key})
            
            turn = 'O' if turn == 'X' else 'X'

        # Assegna ricompense e aggiorna la Q-table
        winner = check_game_over(board)
        reward = 0
        if winner == 'O': reward = 1      # L'agente vince
        elif winner == 'X': reward = -1   # L'agente perde
        
        for step in reversed(history):
            state = step['state']
            action = step['action']
            
            if state not in q_table: q_table[state] = {}
            old_q = q_table[state].get(action, 0.0)
            
            # Formula di base del Q-learning
            q_table[state][action] = old_q + ALPHA * (reward - old_q)
            reward *= GAMMA # La ricompensa diminuisce per le mosse più lontane nel tempo
        
        if (i+1) % 5000 == 0:
            print(f"Episodio {i+1}/{episodes} completato.")

    save_q_table(q_table)
    print("Training completato e Q-table salvata.")


if __name__ == '__main__':
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'train':
        train()
    else:
        if not os.path.exists(Q_TABLE_FILE):
            print("ATTENZIONE: File q_table.json non trovato.")
            print("L'agente si comporterà in modo casuale.")
            print("Esegui 'python server.py train' per addestrarlo.")
        
        print("Avvio del server Flask...")
        app.run(host='0.0.0.0', port=8080)
